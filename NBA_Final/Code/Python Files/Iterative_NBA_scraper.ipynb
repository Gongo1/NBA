{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iterative NBA_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsnoWmsQ-2yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McN7dsL0jtLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup as bsp\n",
        "import requests\n",
        "import pandas as pd\n",
        "import names\n",
        "import re\n",
        "import urllib\n",
        "import os\n",
        "import time\n",
        "import unicodedata\n",
        "from datetime import datetime\n",
        "start = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi3c4tBKNtc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_articles(base_url):\n",
        "  #----------------------------------------------------\n",
        "  ''' Collect all article links from HTML using base_url '''\n",
        "\n",
        "  # connect to server, randomize user-agent tag to enable prolonged iterations\n",
        "  req = requests.get(base_url, headers=({'User-Agent' : names.get_full_name()}))\n",
        "  # get the html content from the webpage\n",
        "  r = req.content\n",
        "  # 'soup' it\n",
        "  soup = bsp(r, 'lxml')\n",
        "\n",
        "  #We only want 2019 rn \n",
        "  nba_articles = []\n",
        "  for tag in soup.findAll('a', href=True):\n",
        "      nba_articles.append(tag['href'])\n",
        "      #print (tag['href'])\n",
        "      \n",
        "      \n",
        "\n",
        "  #nba_articles[1:1000]\n",
        "\n",
        "  #Getting new links for only articles written about NBA\n",
        "  #Scraping brought up a lot of articles related to espn but not NBA \n",
        "\n",
        "  nba_url = [s for s in nba_articles if 'http://www.espn.com/nba/' in s]\n",
        "  return(nba_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcy_xjYUPGnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_titles(nba_url):\n",
        "  #start = time.time()\n",
        "\n",
        "  #os.chdir(\"interim/\")\n",
        "  df = pd.DataFrame(columns=['Body', 'Date', 'Title'])\n",
        "  name = ['']\n",
        "  for URL in nba_url:\n",
        "      page = requests.get(URL)\n",
        "      soup = bsp(page.content, 'html.parser')\n",
        "      results = soup.find(id='article-feed')\n",
        "      #print(results.prettify())\n",
        "      if results.find_all('div', class_='container') == \"NoneType\":\n",
        "              break\n",
        "      job_elems = results.find_all('div', class_='container')\n",
        "      \n",
        "      for job_elem in job_elems:\n",
        "          # Each job_elem is a new BeautifulSoup object.\n",
        "          # You can use the same methods on it as you did before.\n",
        "          title_elem = job_elem.find('header', class_='article-header')\n",
        "          body_elem = job_elem.find('div', class_='article-body')\n",
        "          date_elem = job_elem.find('span', class_='timestamp')\n",
        "          #make into pandas dataframe to import R \n",
        "          if None in (title_elem, body_elem, date_elem):\n",
        "              continue\n",
        "          # {Title, Date, Body, Link}\n",
        "          print(\"Retrieving information for: \", title_elem.text.strip())\n",
        "          d = {\"Title\": [title_elem.text.strip()], 'Body': [body_elem.text.strip()], \"Date\": [date_elem.text.strip()]}\n",
        "          #making df for R \n",
        "          new_df = pd.DataFrame(data = d)\n",
        "          name = date_elem\n",
        "      name = date_elem\n",
        "      #print(name)\n",
        "      df = df.append(new_df)\n",
        "      return(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NlZ9gZClV4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_articles():\n",
        "  ''' Iteratively obtain URLs '''\n",
        "  current_year = int(datetime.now().year)\n",
        "  url_template = 'http://www.espn.com/nba/news/archive/_/month/{month}/year/{year}'\n",
        "  article_list = []\n",
        "  months_list = ['january','february','march','april','may','june','july','august','september','october','november','december']\n",
        "  df = pd.DataFrame(columns=['Body', 'Date', 'Title'])\n",
        "\n",
        "  for year in range(2003, current_year+1):\n",
        "\n",
        "    for month in months_list:\n",
        "\n",
        "      try:\n",
        "        base_url = url_template.format(month = month,year = year)\n",
        "        nba_url = collect_articles(base_url)\n",
        "        df = df.append(get_titles(nba_url))\n",
        "        #print(month,year)\n",
        "      except:\n",
        "        print(str(month) + str(year) + \"doesn't work\")\n",
        "  \n",
        "  return(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67J-MySL5pLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = get_articles()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfM3SJNQn-qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df.to_csv(\"NBA ESPN Articles.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}